{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f8f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.functions import htmltotext, some_actions, query_url, send_query, save_data, check_new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712f3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d73ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(  executable_path = \"C:\\\\Chrome\\\\chromedriver\" )\n",
    "browser.set_window_size(1920, 1080)\n",
    "action  = webdriver.ActionChains(browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577ad92",
   "metadata": {},
   "source": [
    "### Page to check robots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "browser.set_page_load_timeout(0.55)\n",
    "\n",
    "while i != 1000:\n",
    "    i += 1\n",
    "    try:\n",
    "        browser.get(\"https://www.bloomberg.com/\")\n",
    "        browser.delete_all_cookies()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb019f",
   "metadata": {},
   "source": [
    "# Confirm not a robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe93fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def click_and_hold( clickToX, clickToY, sleep, reset=1 ):\n",
    "    \"\"\" Click and hold several seconds using X and Y coordinates\n",
    "    \n",
    "        Args:\n",
    "            clickToX ([float]) : [x coordinate]\n",
    "            clickToY ([float]) : [y coordinate]\n",
    "            sleep    ([float]) : [hold seconds]\n",
    "            reset    ([int])   : [1 if want to move mouse, -1 if return to default place]\n",
    "    \"\"\"\n",
    "\n",
    "    # if reset is -1, move mouse to default position\n",
    "    clickToX *= reset\n",
    "    clickToY *= reset\n",
    "    \n",
    "    if reset == 1:\n",
    "        action.move_by_offset( clickToX, clickToY)\n",
    "        action.click_and_hold().perform()\n",
    "        time.sleep(sleep)\n",
    "    else:\n",
    "        action.move_by_offset( clickToX, clickToY)\n",
    "        \n",
    "    # delete all actions    \n",
    "    action.reset_actions()\n",
    "\n",
    "def confirm_not_robot():\n",
    "    \n",
    "    # coordinates of robot button    \n",
    "    clickToX = browser.get_window_size()['width']/2 - 150 + random.uniform(-2,2)\n",
    "    clickToY = 250 + random.randint(-1,3) + random.uniform(-2,2)\n",
    "    click_wait_time = 5 + random.uniform(-0.5,0.5)\n",
    "    \n",
    "    # click and hold the robot button\n",
    "    click_and_hold( clickToX, clickToY, click_wait_time, 1 )\n",
    "    click_and_hold( clickToX, clickToY, 0, -1)\n",
    "    time.sleep(click_wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae2939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_not_robot( ):\n",
    "    cnt_capthca = 0\n",
    "    cnt_except  = 0\n",
    "    cnt_need    = 3\n",
    "    \n",
    "    # return if we confirm 3 times and pange still exist \n",
    "    while cnt_capthca != cnt_need and cnt_except != cnt_need :\n",
    "        print( cnt_capthca, cnt_except )\n",
    "        try:\n",
    "            # chekc if confirming captcha in page                \n",
    "            if browser.find_element_by_id( \"px-captcha\" ):\n",
    "                cnt_capthca += 1\n",
    "                confirm_not_robot()\n",
    "        except Exception as e:\n",
    "            cnt_except += 1\n",
    "            if \"no such element\" not in str(e):\n",
    "                print( str(e) )\n",
    "                \n",
    "      \n",
    "    if cnt_capthca == cnt_need :\n",
    "        browser.delete_all_cookies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f053dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_browser():\n",
    "    # if we still robot change browser\n",
    "    global browser, action\n",
    "    browser.close()\n",
    "    browser = webdriver.Chrome(  executable_path = \"C:\\\\Chrome\\\\chromedriver\" )\n",
    "    browser.set_window_size(1920, 1080)\n",
    "    action  = webdriver.ActionChains(browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0503f4c",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b897259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_urls_from_html( articles ):\n",
    "    \"\"\" Get titles and hrefs to articles\n",
    "    \n",
    "        Args: \n",
    "            articles ([string]) : [html string with articles hrefs and titles]\n",
    "            \n",
    "        Return:\n",
    "            [dict] : [dict of articles (url:title)]\n",
    "    \"\"\"\n",
    "\n",
    "    articles_url_name = {}\n",
    "    \n",
    "    \n",
    "    for article in articles[:5]:\n",
    "        \n",
    "        # remove the trend position from href ( http://bloomberg.com/article/14?13 )\n",
    "        if \"?\" in article['href']:\n",
    "            article['href'] = article['href'].split(\"?\")[0]\n",
    "            \n",
    "        href = source_url + article['href']\n",
    "        articles_url_name[ href ] = article.get_text()\n",
    "        \n",
    "    return articles_url_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93025ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_category( url, cnt=0 ):\n",
    "    \"\"\" Collect articles urls using page url\n",
    "        \n",
    "        Args:\n",
    "            url ([string]) : [the url of the page]\n",
    "            cnt ([int, default 0]) : [count of recursion calls]\n",
    "            \n",
    "        Return:\n",
    "            [dict] : [dict of articles (url:title)]\n",
    "    \"\"\"\n",
    "\n",
    "    # getting source html of the page\n",
    "    html = query_url(url, browser)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # if this element in page, all right ( html atribute with articles info )\n",
    "    compare_element = soup.find_all( \"a\", attrs={\"class\" : \"story-package-module__story__headline-link\"} )\n",
    "    \n",
    "    # if articles doesn't exist try to confirm that not a robot   \n",
    "    if  len( compare_element ) == 0 and cnt != 1 :\n",
    "        process_not_robot()\n",
    "        return get_urls_from_category( url, cnt=1 )\n",
    "    # if we already try to confirm change browser    \n",
    "    elif len( compare_element ) == 0 and cnt == 1:\n",
    "        change_browser()\n",
    "        return {}\n",
    "    \n",
    "    articles = compare_element\n",
    "    \n",
    "    return get_article_urls_from_html( articles )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e741196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content( url, cnt=0 ):\n",
    "    \"\"\" Collect article content by url\n",
    "    \n",
    "        Args:\n",
    "            url ([string]) : [article url]\n",
    "            \n",
    "        Return:\n",
    "            [string article content, string article time]\n",
    "    \"\"\"\n",
    "    \n",
    "    # geting sourse html of the page\n",
    "    html = query_url(url, browser)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # if this element in page, all right    \n",
    "    compare_element = soup.find_all( attrs={\"class\" : \"body-copy-v2\"} )\n",
    "    \n",
    "    # if articles doesn't exist try to confirm that not a robot   \n",
    "    if  len( compare_element ) == 0 and cnt != 1 :\n",
    "        process_not_robot()\n",
    "        return get_article_content( url, cnt=1 )\n",
    "    # if we already try to confirm change browser \n",
    "    elif len( compare_element ) == 0 and cnt == 1:\n",
    "        change_browser()\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    # get free content ( pre_wiev), and content from paywall    \n",
    "    pre_wiev = compare_element\n",
    "    content  = soup.find_all( \"p\", attrs={\"class\" : \"paywall\"} )\n",
    "\n",
    "    # get remaining text\n",
    "    text_cont = \" \".join( [ element.get_text() for element in content ] ) \n",
    "    \n",
    "    try:\n",
    "        text = \" \".join( [ element.get_text() for element in pre_wiev ] ) \n",
    "        # get first sentense, since its prewiev\n",
    "        text = sent_tokenize(text)[0]\n",
    "        text_cont = text + text_cont\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        print( str(e) ) \n",
    "\n",
    "    text_cont = htmltotext( text_cont )\n",
    "    \n",
    "    # get article time\n",
    "    time = soup.find( attrs={\"class\" : \"article-timestamp\"} ).get_text()\n",
    "    \n",
    "    return text_cont, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357c8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_process( category, category_articles_data, sleep_between_url , url, url_server ):\n",
    "    \"\"\" Scraping process. From url get new articles. From articles get content\n",
    "    \n",
    "        Args:\n",
    "            category ([string]) : [Which category will collect from]\n",
    "            category_articles_data ([pandas.DataFrame]) : [data frame with columns [\"url\", \"title\", \"text\", \"date\", \"category\"] ]\n",
    "            sleep_between_url ([float]) : [sleep time between each url]\n",
    "            url_server ([string]) : [where send article data]\n",
    "    \"\"\"\n",
    "    \n",
    "    # check new articles    \n",
    "    articles_url_name_new = get_urls_from_category( url )\n",
    "    articles_old_urls = category_articles_data[\"url\"].values\n",
    "    new_articles_urls = check_new_articles( list( articles_url_name_new.keys() ) , articles_old_urls )\n",
    "    \n",
    "    # if appear new article. get content\n",
    "    for article_url in new_articles_urls:\n",
    "        \n",
    "        text_cont, time_art = get_article_content( article_url )\n",
    "        \n",
    "        # create new article data\n",
    "        new_data = {\n",
    "            \"url\"      : article_url,\n",
    "            \"title\"    : articles_url_name_new[article_url],\n",
    "            \"text\"     : text_cont,\n",
    "            \"date\"     : time_art,\n",
    "            \"category\" : category\n",
    "        }\n",
    "        category_articles_data = category_articles_data.append( new_data, ignore_index=True )\n",
    "        \n",
    "        # each 10 article pickle\n",
    "        if category_articles_data.shape[0] % 10 == 0:\n",
    "            pickle.dump( category_articles_data , open(pickle_obj_path,\"wb\")) \n",
    "        # if article counts > 300, save 150        \n",
    "        if category_articles_data.shape[0] > 300:\n",
    "            category_articles_data = save_data( category_articles_data, df_save_path )\n",
    "            pickle.dump( category_articles_data , open(pickle_obj_path,\"wb\")) \n",
    "            \n",
    "        # send data to server\n",
    "        try:\n",
    "            send_query( url_server, new_data )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep( sleep_between_url )\n",
    "    \n",
    "    # dump after each category since articles can repeat    \n",
    "    pickle.dump( category_articles_data , open(pickle_obj_path,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "086f24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorys = [ \"/market\", \"/technology\", \"/politics\", \"/markets/economics\"]\n",
    "df_columns = [\"url\", \"title\", \"text\", \"date\", \"category\"]\n",
    "\n",
    "url_server = \"http://localhost:9000/\"\n",
    "source_url = \"https://www.bloomberg.com\"\n",
    "pickle_obj_path = \"bloomberg_data.pkl\"\n",
    "df_save_path = \"./Bloomberg/bloomberg_data\"\n",
    "\n",
    "sleep_between_url = 3\n",
    "sleep_between_cat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476cf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_articles_data = pd.DataFrame( columns = df_columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326529d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_articles_data = pickle.load(open(pickle_obj_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ff531",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    for category in categorys:\n",
    "\n",
    "        url = source_url + category\n",
    "        category_articles_data = pickle.load(open(pickle_obj_path, \"rb\"))\n",
    "        \n",
    "        try:\n",
    "            main_process( category, category_articles_data, sleep_between_url, url, url_server )\n",
    "        except:\n",
    "            change_browser()\n",
    "        \n",
    "        time.sleep(sleep_between_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c6b987",
   "metadata": {},
   "source": [
    "# Save and Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5851d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( category_articles_data , open(pickle_obj_path,\"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d17415",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_articles_data = pickle.load(open(pickle_obj_path, \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
